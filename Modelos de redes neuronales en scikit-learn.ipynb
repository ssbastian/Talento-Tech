{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNH+WojJuULU93USWrbJWHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssbastian/Talento-Tech/blob/main/Modelos%20de%20redes%20neuronales%20en%20scikit-learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###** Juan Sebastian Sanchez **\n"
      ],
      "metadata": {
        "id": "-SPjIO7plwT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpcT7u64gNgF"
      },
      "outputs": [],
      "source": [
        "#load the libraries\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "iris = load_iris()\n",
        "x, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "GsTLpEeYglb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explore the data\n",
        "type(iris)\n",
        "iris.keys()\n",
        "iris['data']\n",
        "iris['target']\n",
        "iris['target_names']\n",
        "iris['DESCR']\n",
        "iris['feature_names']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-Zz5BsJhgzk",
        "outputId": "fcd942be-1af2-4fb1-ddbb-e4cd1eef295b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "rf2O9I-iiHgu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaler features for better performance\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "Xgg0agRLiu-Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create MLP classifier instance with hidden layer of 100 neurons\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=(100),\n",
        "                        activation='relu',\n",
        "                        solver='adam',\n",
        "                        max_iter=500,\n",
        "                        random_state=42,\n",
        "                        verbose=True)"
      ],
      "metadata": {
        "id": "IhtYWNOUjRbn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit of model\n",
        "mlp_clf.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cSWgJL0ukMOU",
        "outputId": "6594e178-8eef-4f2f-e2cf-886f53a52376"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.16374923\n",
            "Iteration 2, loss = 1.14090611\n",
            "Iteration 3, loss = 1.11857816\n",
            "Iteration 4, loss = 1.09677375\n",
            "Iteration 5, loss = 1.07547149\n",
            "Iteration 6, loss = 1.05468504\n",
            "Iteration 7, loss = 1.03440573\n",
            "Iteration 8, loss = 1.01462373\n",
            "Iteration 9, loss = 0.99535820\n",
            "Iteration 10, loss = 0.97658926\n",
            "Iteration 11, loss = 0.95833257\n",
            "Iteration 12, loss = 0.94053852\n",
            "Iteration 13, loss = 0.92322169\n",
            "Iteration 14, loss = 0.90638373\n",
            "Iteration 15, loss = 0.89000670\n",
            "Iteration 16, loss = 0.87408395\n",
            "Iteration 17, loss = 0.85861030\n",
            "Iteration 18, loss = 0.84357120\n",
            "Iteration 19, loss = 0.82895431\n",
            "Iteration 20, loss = 0.81476466\n",
            "Iteration 21, loss = 0.80097410\n",
            "Iteration 22, loss = 0.78759157\n",
            "Iteration 23, loss = 0.77460302\n",
            "Iteration 24, loss = 0.76199129\n",
            "Iteration 25, loss = 0.74975377\n",
            "Iteration 26, loss = 0.73786826\n",
            "Iteration 27, loss = 0.72632581\n",
            "Iteration 28, loss = 0.71511871\n",
            "Iteration 29, loss = 0.70424319\n",
            "Iteration 30, loss = 0.69368183\n",
            "Iteration 31, loss = 0.68342580\n",
            "Iteration 32, loss = 0.67346953\n",
            "Iteration 33, loss = 0.66380608\n",
            "Iteration 34, loss = 0.65442161\n",
            "Iteration 35, loss = 0.64531278\n",
            "Iteration 36, loss = 0.63647902\n",
            "Iteration 37, loss = 0.62790342\n",
            "Iteration 38, loss = 0.61957863\n",
            "Iteration 39, loss = 0.61148527\n",
            "Iteration 40, loss = 0.60361466\n",
            "Iteration 41, loss = 0.59597076\n",
            "Iteration 42, loss = 0.58855743\n",
            "Iteration 43, loss = 0.58136358\n",
            "Iteration 44, loss = 0.57437150\n",
            "Iteration 45, loss = 0.56758016\n",
            "Iteration 46, loss = 0.56097952\n",
            "Iteration 47, loss = 0.55455113\n",
            "Iteration 48, loss = 0.54829175\n",
            "Iteration 49, loss = 0.54219766\n",
            "Iteration 50, loss = 0.53626159\n",
            "Iteration 51, loss = 0.53048210\n",
            "Iteration 52, loss = 0.52484566\n",
            "Iteration 53, loss = 0.51933866\n",
            "Iteration 54, loss = 0.51394846\n",
            "Iteration 55, loss = 0.50869354\n",
            "Iteration 56, loss = 0.50356710\n",
            "Iteration 57, loss = 0.49857160\n",
            "Iteration 58, loss = 0.49370008\n",
            "Iteration 59, loss = 0.48894487\n",
            "Iteration 60, loss = 0.48429908\n",
            "Iteration 61, loss = 0.47974886\n",
            "Iteration 62, loss = 0.47529331\n",
            "Iteration 63, loss = 0.47093384\n",
            "Iteration 64, loss = 0.46667067\n",
            "Iteration 65, loss = 0.46249978\n",
            "Iteration 66, loss = 0.45841914\n",
            "Iteration 67, loss = 0.45442291\n",
            "Iteration 68, loss = 0.45050855\n",
            "Iteration 69, loss = 0.44667746\n",
            "Iteration 70, loss = 0.44292801\n",
            "Iteration 71, loss = 0.43926405\n",
            "Iteration 72, loss = 0.43567120\n",
            "Iteration 73, loss = 0.43215951\n",
            "Iteration 74, loss = 0.42871948\n",
            "Iteration 75, loss = 0.42534205\n",
            "Iteration 76, loss = 0.42201917\n",
            "Iteration 77, loss = 0.41875279\n",
            "Iteration 78, loss = 0.41554234\n",
            "Iteration 79, loss = 0.41238463\n",
            "Iteration 80, loss = 0.40928512\n",
            "Iteration 81, loss = 0.40623752\n",
            "Iteration 82, loss = 0.40323632\n",
            "Iteration 83, loss = 0.40027444\n",
            "Iteration 84, loss = 0.39735268\n",
            "Iteration 85, loss = 0.39447038\n",
            "Iteration 86, loss = 0.39162936\n",
            "Iteration 87, loss = 0.38882549\n",
            "Iteration 88, loss = 0.38605547\n",
            "Iteration 89, loss = 0.38331954\n",
            "Iteration 90, loss = 0.38062197\n",
            "Iteration 91, loss = 0.37796519\n",
            "Iteration 92, loss = 0.37534717\n",
            "Iteration 93, loss = 0.37276083\n",
            "Iteration 94, loss = 0.37020149\n",
            "Iteration 95, loss = 0.36766960\n",
            "Iteration 96, loss = 0.36516474\n",
            "Iteration 97, loss = 0.36267665\n",
            "Iteration 98, loss = 0.36021595\n",
            "Iteration 99, loss = 0.35777846\n",
            "Iteration 100, loss = 0.35536449\n",
            "Iteration 101, loss = 0.35297445\n",
            "Iteration 102, loss = 0.35060974\n",
            "Iteration 103, loss = 0.34826529\n",
            "Iteration 104, loss = 0.34593483\n",
            "Iteration 105, loss = 0.34362473\n",
            "Iteration 106, loss = 0.34133717\n",
            "Iteration 107, loss = 0.33906613\n",
            "Iteration 108, loss = 0.33680592\n",
            "Iteration 109, loss = 0.33456218\n",
            "Iteration 110, loss = 0.33233236\n",
            "Iteration 111, loss = 0.33011684\n",
            "Iteration 112, loss = 0.32791283\n",
            "Iteration 113, loss = 0.32571695\n",
            "Iteration 114, loss = 0.32353976\n",
            "Iteration 115, loss = 0.32137787\n",
            "Iteration 116, loss = 0.31923092\n",
            "Iteration 117, loss = 0.31709830\n",
            "Iteration 118, loss = 0.31498124\n",
            "Iteration 119, loss = 0.31287552\n",
            "Iteration 120, loss = 0.31078247\n",
            "Iteration 121, loss = 0.30870180\n",
            "Iteration 122, loss = 0.30663215\n",
            "Iteration 123, loss = 0.30457529\n",
            "Iteration 124, loss = 0.30253115\n",
            "Iteration 125, loss = 0.30050602\n",
            "Iteration 126, loss = 0.29849435\n",
            "Iteration 127, loss = 0.29649447\n",
            "Iteration 128, loss = 0.29450772\n",
            "Iteration 129, loss = 0.29253428\n",
            "Iteration 130, loss = 0.29057476\n",
            "Iteration 131, loss = 0.28862124\n",
            "Iteration 132, loss = 0.28667327\n",
            "Iteration 133, loss = 0.28473682\n",
            "Iteration 134, loss = 0.28281391\n",
            "Iteration 135, loss = 0.28090420\n",
            "Iteration 136, loss = 0.27900732\n",
            "Iteration 137, loss = 0.27712298\n",
            "Iteration 138, loss = 0.27525135\n",
            "Iteration 139, loss = 0.27338685\n",
            "Iteration 140, loss = 0.27152667\n",
            "Iteration 141, loss = 0.26967986\n",
            "Iteration 142, loss = 0.26785020\n",
            "Iteration 143, loss = 0.26603105\n",
            "Iteration 144, loss = 0.26422269\n",
            "Iteration 145, loss = 0.26242392\n",
            "Iteration 146, loss = 0.26063838\n",
            "Iteration 147, loss = 0.25886561\n",
            "Iteration 148, loss = 0.25710379\n",
            "Iteration 149, loss = 0.25535207\n",
            "Iteration 150, loss = 0.25361084\n",
            "Iteration 151, loss = 0.25187929\n",
            "Iteration 152, loss = 0.25015813\n",
            "Iteration 153, loss = 0.24844864\n",
            "Iteration 154, loss = 0.24675070\n",
            "Iteration 155, loss = 0.24506624\n",
            "Iteration 156, loss = 0.24339440\n",
            "Iteration 157, loss = 0.24173266\n",
            "Iteration 158, loss = 0.24008142\n",
            "Iteration 159, loss = 0.23844062\n",
            "Iteration 160, loss = 0.23680994\n",
            "Iteration 161, loss = 0.23518841\n",
            "Iteration 162, loss = 0.23357753\n",
            "Iteration 163, loss = 0.23197948\n",
            "Iteration 164, loss = 0.23039020\n",
            "Iteration 165, loss = 0.22881078\n",
            "Iteration 166, loss = 0.22724050\n",
            "Iteration 167, loss = 0.22567998\n",
            "Iteration 168, loss = 0.22412789\n",
            "Iteration 169, loss = 0.22258465\n",
            "Iteration 170, loss = 0.22105151\n",
            "Iteration 171, loss = 0.21953026\n",
            "Iteration 172, loss = 0.21801982\n",
            "Iteration 173, loss = 0.21651969\n",
            "Iteration 174, loss = 0.21503013\n",
            "Iteration 175, loss = 0.21355071\n",
            "Iteration 176, loss = 0.21208168\n",
            "Iteration 177, loss = 0.21062231\n",
            "Iteration 178, loss = 0.20917342\n",
            "Iteration 179, loss = 0.20773554\n",
            "Iteration 180, loss = 0.20630868\n",
            "Iteration 181, loss = 0.20489298\n",
            "Iteration 182, loss = 0.20348724\n",
            "Iteration 183, loss = 0.20209246\n",
            "Iteration 184, loss = 0.20070906\n",
            "Iteration 185, loss = 0.19933550\n",
            "Iteration 186, loss = 0.19797120\n",
            "Iteration 187, loss = 0.19662322\n",
            "Iteration 188, loss = 0.19528742\n",
            "Iteration 189, loss = 0.19396136\n",
            "Iteration 190, loss = 0.19264477\n",
            "Iteration 191, loss = 0.19134128\n",
            "Iteration 192, loss = 0.19004939\n",
            "Iteration 193, loss = 0.18876562\n",
            "Iteration 194, loss = 0.18749325\n",
            "Iteration 195, loss = 0.18623117\n",
            "Iteration 196, loss = 0.18497756\n",
            "Iteration 197, loss = 0.18373388\n",
            "Iteration 198, loss = 0.18250042\n",
            "Iteration 199, loss = 0.18127737\n",
            "Iteration 200, loss = 0.18006333\n",
            "Iteration 201, loss = 0.17885751\n",
            "Iteration 202, loss = 0.17766084\n",
            "Iteration 203, loss = 0.17647421\n",
            "Iteration 204, loss = 0.17529673\n",
            "Iteration 205, loss = 0.17413009\n",
            "Iteration 206, loss = 0.17297380\n",
            "Iteration 207, loss = 0.17182701\n",
            "Iteration 208, loss = 0.17069045\n",
            "Iteration 209, loss = 0.16956423\n",
            "Iteration 210, loss = 0.16844842\n",
            "Iteration 211, loss = 0.16734069\n",
            "Iteration 212, loss = 0.16623880\n",
            "Iteration 213, loss = 0.16514569\n",
            "Iteration 214, loss = 0.16406033\n",
            "Iteration 215, loss = 0.16298241\n",
            "Iteration 216, loss = 0.16191279\n",
            "Iteration 217, loss = 0.16085277\n",
            "Iteration 218, loss = 0.15980164\n",
            "Iteration 219, loss = 0.15876016\n",
            "Iteration 220, loss = 0.15772894\n",
            "Iteration 221, loss = 0.15670742\n",
            "Iteration 222, loss = 0.15569736\n",
            "Iteration 223, loss = 0.15469586\n",
            "Iteration 224, loss = 0.15370331\n",
            "Iteration 225, loss = 0.15272028\n",
            "Iteration 226, loss = 0.15174884\n",
            "Iteration 227, loss = 0.15078805\n",
            "Iteration 228, loss = 0.14983877\n",
            "Iteration 229, loss = 0.14889923\n",
            "Iteration 230, loss = 0.14796880\n",
            "Iteration 231, loss = 0.14704769\n",
            "Iteration 232, loss = 0.14613555\n",
            "Iteration 233, loss = 0.14523174\n",
            "Iteration 234, loss = 0.14433755\n",
            "Iteration 235, loss = 0.14345342\n",
            "Iteration 236, loss = 0.14257887\n",
            "Iteration 237, loss = 0.14171024\n",
            "Iteration 238, loss = 0.14085024\n",
            "Iteration 239, loss = 0.13999872\n",
            "Iteration 240, loss = 0.13915635\n",
            "Iteration 241, loss = 0.13832276\n",
            "Iteration 242, loss = 0.13749751\n",
            "Iteration 243, loss = 0.13668175\n",
            "Iteration 244, loss = 0.13587410\n",
            "Iteration 245, loss = 0.13507462\n",
            "Iteration 246, loss = 0.13428305\n",
            "Iteration 247, loss = 0.13350203\n",
            "Iteration 248, loss = 0.13272974\n",
            "Iteration 249, loss = 0.13196522\n",
            "Iteration 250, loss = 0.13120917\n",
            "Iteration 251, loss = 0.13046105\n",
            "Iteration 252, loss = 0.12972226\n",
            "Iteration 253, loss = 0.12899130\n",
            "Iteration 254, loss = 0.12826784\n",
            "Iteration 255, loss = 0.12755190\n",
            "Iteration 256, loss = 0.12684324\n",
            "Iteration 257, loss = 0.12614193\n",
            "Iteration 258, loss = 0.12544776\n",
            "Iteration 259, loss = 0.12476104\n",
            "Iteration 260, loss = 0.12408139\n",
            "Iteration 261, loss = 0.12340901\n",
            "Iteration 262, loss = 0.12274314\n",
            "Iteration 263, loss = 0.12208312\n",
            "Iteration 264, loss = 0.12142958\n",
            "Iteration 265, loss = 0.12078307\n",
            "Iteration 266, loss = 0.12014329\n",
            "Iteration 267, loss = 0.11950997\n",
            "Iteration 268, loss = 0.11888279\n",
            "Iteration 269, loss = 0.11826228\n",
            "Iteration 270, loss = 0.11764849\n",
            "Iteration 271, loss = 0.11704092\n",
            "Iteration 272, loss = 0.11643983\n",
            "Iteration 273, loss = 0.11584425\n",
            "Iteration 274, loss = 0.11525452\n",
            "Iteration 275, loss = 0.11467096\n",
            "Iteration 276, loss = 0.11409323\n",
            "Iteration 277, loss = 0.11352159\n",
            "Iteration 278, loss = 0.11295583\n",
            "Iteration 279, loss = 0.11239614\n",
            "Iteration 280, loss = 0.11184404\n",
            "Iteration 281, loss = 0.11129860\n",
            "Iteration 282, loss = 0.11075959\n",
            "Iteration 283, loss = 0.11022598\n",
            "Iteration 284, loss = 0.10969810\n",
            "Iteration 285, loss = 0.10917615\n",
            "Iteration 286, loss = 0.10865943\n",
            "Iteration 287, loss = 0.10814853\n",
            "Iteration 288, loss = 0.10764284\n",
            "Iteration 289, loss = 0.10714313\n",
            "Iteration 290, loss = 0.10664900\n",
            "Iteration 291, loss = 0.10616008\n",
            "Iteration 292, loss = 0.10567640\n",
            "Iteration 293, loss = 0.10519768\n",
            "Iteration 294, loss = 0.10472433\n",
            "Iteration 295, loss = 0.10425596\n",
            "Iteration 296, loss = 0.10379243\n",
            "Iteration 297, loss = 0.10333471\n",
            "Iteration 298, loss = 0.10288192\n",
            "Iteration 299, loss = 0.10243412\n",
            "Iteration 300, loss = 0.10199090\n",
            "Iteration 301, loss = 0.10155249\n",
            "Iteration 302, loss = 0.10111871\n",
            "Iteration 303, loss = 0.10068923\n",
            "Iteration 304, loss = 0.10026394\n",
            "Iteration 305, loss = 0.09984273\n",
            "Iteration 306, loss = 0.09942673\n",
            "Iteration 307, loss = 0.09901547\n",
            "Iteration 308, loss = 0.09860866\n",
            "Iteration 309, loss = 0.09820570\n",
            "Iteration 310, loss = 0.09780734\n",
            "Iteration 311, loss = 0.09741315\n",
            "Iteration 312, loss = 0.09702281\n",
            "Iteration 313, loss = 0.09663715\n",
            "Iteration 314, loss = 0.09625569\n",
            "Iteration 315, loss = 0.09587798\n",
            "Iteration 316, loss = 0.09550569\n",
            "Iteration 317, loss = 0.09513735\n",
            "Iteration 318, loss = 0.09477232\n",
            "Iteration 319, loss = 0.09441128\n",
            "Iteration 320, loss = 0.09405357\n",
            "Iteration 321, loss = 0.09369931\n",
            "Iteration 322, loss = 0.09334863\n",
            "Iteration 323, loss = 0.09300135\n",
            "Iteration 324, loss = 0.09265738\n",
            "Iteration 325, loss = 0.09231665\n",
            "Iteration 326, loss = 0.09197943\n",
            "Iteration 327, loss = 0.09164580\n",
            "Iteration 328, loss = 0.09131548\n",
            "Iteration 329, loss = 0.09098827\n",
            "Iteration 330, loss = 0.09066436\n",
            "Iteration 331, loss = 0.09034366\n",
            "Iteration 332, loss = 0.09002601\n",
            "Iteration 333, loss = 0.08971144\n",
            "Iteration 334, loss = 0.08939984\n",
            "Iteration 335, loss = 0.08909118\n",
            "Iteration 336, loss = 0.08878572\n",
            "Iteration 337, loss = 0.08848301\n",
            "Iteration 338, loss = 0.08818292\n",
            "Iteration 339, loss = 0.08788556\n",
            "Iteration 340, loss = 0.08759094\n",
            "Iteration 341, loss = 0.08729898\n",
            "Iteration 342, loss = 0.08700965\n",
            "Iteration 343, loss = 0.08672303\n",
            "Iteration 344, loss = 0.08643914\n",
            "Iteration 345, loss = 0.08615823\n",
            "Iteration 346, loss = 0.08588028\n",
            "Iteration 347, loss = 0.08560477\n",
            "Iteration 348, loss = 0.08533161\n",
            "Iteration 349, loss = 0.08506079\n",
            "Iteration 350, loss = 0.08479251\n",
            "Iteration 351, loss = 0.08452663\n",
            "Iteration 352, loss = 0.08426321\n",
            "Iteration 353, loss = 0.08400213\n",
            "Iteration 354, loss = 0.08374340\n",
            "Iteration 355, loss = 0.08348703\n",
            "Iteration 356, loss = 0.08323280\n",
            "Iteration 357, loss = 0.08298086\n",
            "Iteration 358, loss = 0.08273117\n",
            "Iteration 359, loss = 0.08248365\n",
            "Iteration 360, loss = 0.08223812\n",
            "Iteration 361, loss = 0.08199446\n",
            "Iteration 362, loss = 0.08175283\n",
            "Iteration 363, loss = 0.08151410\n",
            "Iteration 364, loss = 0.08127748\n",
            "Iteration 365, loss = 0.08104312\n",
            "Iteration 366, loss = 0.08081095\n",
            "Iteration 367, loss = 0.08058042\n",
            "Iteration 368, loss = 0.08035177\n",
            "Iteration 369, loss = 0.08012510\n",
            "Iteration 370, loss = 0.07990007\n",
            "Iteration 371, loss = 0.07967725\n",
            "Iteration 372, loss = 0.07945626\n",
            "Iteration 373, loss = 0.07923702\n",
            "Iteration 374, loss = 0.07901948\n",
            "Iteration 375, loss = 0.07880337\n",
            "Iteration 376, loss = 0.07858964\n",
            "Iteration 377, loss = 0.07837765\n",
            "Iteration 378, loss = 0.07816737\n",
            "Iteration 379, loss = 0.07795870\n",
            "Iteration 380, loss = 0.07775158\n",
            "Iteration 381, loss = 0.07754614\n",
            "Iteration 382, loss = 0.07734250\n",
            "Iteration 383, loss = 0.07714026\n",
            "Iteration 384, loss = 0.07693975\n",
            "Iteration 385, loss = 0.07674133\n",
            "Iteration 386, loss = 0.07654435\n",
            "Iteration 387, loss = 0.07634885\n",
            "Iteration 388, loss = 0.07615487\n",
            "Iteration 389, loss = 0.07596245\n",
            "Iteration 390, loss = 0.07577168\n",
            "Iteration 391, loss = 0.07558248\n",
            "Iteration 392, loss = 0.07539467\n",
            "Iteration 393, loss = 0.07520869\n",
            "Iteration 394, loss = 0.07502403\n",
            "Iteration 395, loss = 0.07484074\n",
            "Iteration 396, loss = 0.07465896\n",
            "Iteration 397, loss = 0.07447873\n",
            "Iteration 398, loss = 0.07429994\n",
            "Iteration 399, loss = 0.07412256\n",
            "Iteration 400, loss = 0.07394664\n",
            "Iteration 401, loss = 0.07377194\n",
            "Iteration 402, loss = 0.07359831\n",
            "Iteration 403, loss = 0.07342611\n",
            "Iteration 404, loss = 0.07325569\n",
            "Iteration 405, loss = 0.07308674\n",
            "Iteration 406, loss = 0.07291888\n",
            "Iteration 407, loss = 0.07275219\n",
            "Iteration 408, loss = 0.07258729\n",
            "Iteration 409, loss = 0.07242353\n",
            "Iteration 410, loss = 0.07226085\n",
            "Iteration 411, loss = 0.07209931\n",
            "Iteration 412, loss = 0.07193891\n",
            "Iteration 413, loss = 0.07177973\n",
            "Iteration 414, loss = 0.07162168\n",
            "Iteration 415, loss = 0.07146518\n",
            "Iteration 416, loss = 0.07130976\n",
            "Iteration 417, loss = 0.07115531\n",
            "Iteration 418, loss = 0.07100209\n",
            "Iteration 419, loss = 0.07085013\n",
            "Iteration 420, loss = 0.07069940\n",
            "Iteration 421, loss = 0.07054971\n",
            "Iteration 422, loss = 0.07040103\n",
            "Iteration 423, loss = 0.07025331\n",
            "Iteration 424, loss = 0.07010660\n",
            "Iteration 425, loss = 0.06996119\n",
            "Iteration 426, loss = 0.06981683\n",
            "Iteration 427, loss = 0.06967334\n",
            "Iteration 428, loss = 0.06953078\n",
            "Iteration 429, loss = 0.06938946\n",
            "Iteration 430, loss = 0.06924907\n",
            "Iteration 431, loss = 0.06910964\n",
            "Iteration 432, loss = 0.06897129\n",
            "Iteration 433, loss = 0.06883403\n",
            "Iteration 434, loss = 0.06869763\n",
            "Iteration 435, loss = 0.06856231\n",
            "Iteration 436, loss = 0.06842797\n",
            "Iteration 437, loss = 0.06829448\n",
            "Iteration 438, loss = 0.06816203\n",
            "Iteration 439, loss = 0.06803047\n",
            "Iteration 440, loss = 0.06789983\n",
            "Iteration 441, loss = 0.06777001\n",
            "Iteration 442, loss = 0.06764130\n",
            "Iteration 443, loss = 0.06751344\n",
            "Iteration 444, loss = 0.06738639\n",
            "Iteration 445, loss = 0.06726024\n",
            "Iteration 446, loss = 0.06713501\n",
            "Iteration 447, loss = 0.06701060\n",
            "Iteration 448, loss = 0.06688704\n",
            "Iteration 449, loss = 0.06676437\n",
            "Iteration 450, loss = 0.06664258\n",
            "Iteration 451, loss = 0.06652155\n",
            "Iteration 452, loss = 0.06640145\n",
            "Iteration 453, loss = 0.06628211\n",
            "Iteration 454, loss = 0.06616370\n",
            "Iteration 455, loss = 0.06604692\n",
            "Iteration 456, loss = 0.06593080\n",
            "Iteration 457, loss = 0.06581544\n",
            "Iteration 458, loss = 0.06570079\n",
            "Iteration 459, loss = 0.06558677\n",
            "Iteration 460, loss = 0.06547347\n",
            "Iteration 461, loss = 0.06536102\n",
            "Iteration 462, loss = 0.06524929\n",
            "Iteration 463, loss = 0.06513826\n",
            "Iteration 464, loss = 0.06502789\n",
            "Iteration 465, loss = 0.06491835\n",
            "Iteration 466, loss = 0.06480943\n",
            "Iteration 467, loss = 0.06470113\n",
            "Iteration 468, loss = 0.06459399\n",
            "Iteration 469, loss = 0.06448765\n",
            "Iteration 470, loss = 0.06438184\n",
            "Iteration 471, loss = 0.06427690\n",
            "Iteration 472, loss = 0.06417260\n",
            "Iteration 473, loss = 0.06406882\n",
            "Iteration 474, loss = 0.06396548\n",
            "Iteration 475, loss = 0.06386296\n",
            "Iteration 476, loss = 0.06376117\n",
            "Iteration 477, loss = 0.06366023\n",
            "Iteration 478, loss = 0.06355997\n",
            "Iteration 479, loss = 0.06346030\n",
            "Iteration 480, loss = 0.06336120\n",
            "Iteration 481, loss = 0.06326332\n",
            "Iteration 482, loss = 0.06316624\n",
            "Iteration 483, loss = 0.06306984\n",
            "Iteration 484, loss = 0.06297404\n",
            "Iteration 485, loss = 0.06287884\n",
            "Iteration 486, loss = 0.06278439\n",
            "Iteration 487, loss = 0.06269074\n",
            "Iteration 488, loss = 0.06259756\n",
            "Iteration 489, loss = 0.06250468\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=100, max_iter=500, random_state=42,\n",
              "              verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=500, random_state=42,\n",
              "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=100, max_iter=500, random_state=42,\n",
              "              verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #make predictions on the test set\n",
        "y_pred = mlp_clf.predict(x_test)\n",
        "print(y_test)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyKrg6v9kWlM",
        "outputId": "fe9b420c-d9fe-4fcd-cf85-85172e4d7df8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate precision using the function accuracy store\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precision del modelo:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYMp_5sClBhB",
        "outputId": "05f2da8a-fc16-43bf-816e-639c0ad7b509"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision del modelo: 1.0\n"
          ]
        }
      ]
    }
  ]
}